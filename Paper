# Enhancing LLM Memory with Hippocampal Cognitive Maps: A Neuroscience-Inspired Approach

Tyler Bessire - Lead Researcher 
May 2025

## Abstract

Large Language Models (LLMs) have achieved remarkable success in natural language generation and reasoning, yet they face fundamental limits in memory and context length. While current approaches like expanding context windows and retrieval-augmented generation provide partial solutions, they lack the flexible, structured, and persistent memory capabilities observed in human cognition. This paper draws inspiration from recent neuroscience findings on hippocampal cognitive maps - particularly the discovery that the human brain organizes action-outcome associations in map-like representations - to propose a novel memory architecture for LLMs. We analyze limitations in current LLM memory systems, explain the neuroscience of hippocampal-entorhinal cognitive maps, and present a computational framework that organizes knowledge in a relational space for improved semantic retrieval, contextual adaptability, and long-term retention. Our hippocampal-inspired approach enables one-shot learning of new experiences, structured representation of relationships, and efficient action-outcome mapping for planning. We compare this approach against leading models (GPT-4.1, Claude 3.7, Gemini 2.5), integrate insights from reinforcement learning and multi-agent systems, provide implementation examples with pseudocode, and outline evaluation methodologies. This work establishes a roadmap for memory-enhanced LLMs that better mirror the remarkable capabilities of human memory, potentially leading to AI systems with greater coherence, consistency, and adaptive intelligence over extended interactions.

## Introduction

Large Language Models (LLMs) have achieved remarkable success in natural language generation and reasoning, yet they face fundamental limits in memory and context length. Standard transformer models can only attend to a fixed window of recent tokens, causing information outside this window to be "forgotten" during generation. While recent models have made significant advances in context length - from GPT-4.1's 1 million token context window to Gemini 2.5 Pro's similar capacities - they still face fundamental constraints. These advances allow LLMs to ingest entire books or multi-day dialogues in one prompt. However, simply scaling up context windows is not a panacea: extremely large contexts incur heavy computational cost and can even dilute the model's focus on relevant details. As OpenAI's own tests show, GPT-4.1's accuracy decreases from around 84% with 8,000 tokens to 50% with 1 million tokens. Moreover, all information is treated as a flat sequence, lacking any structured long-term storage or fast retrieval mechanism beyond brute-force attention scan.

Humans and animals, by contrast, excel at long-term memory, recalling specific past events (episodic memory) and factual knowledge (semantic memory) over a lifetime. The brain's hippocampal system plays a key role in rapidly forming new memories and organizing them for later retrieval. Recent neuroscience findings indicate that the hippocampus doesn't store items as unrelated bits, but rather constructs "cognitive maps" – structured mental models of relationships in an environment. In particular, Barnaveli et al. (2025) showed that the human hippocampal-entorhinal network encodes even abstract action-outcome associations in a map-like representation, supporting flexible planning. These insights raise an intriguing question: Can we design LLM memory systems inspired by hippocampal cognitive maps to achieve more adaptive, long-term memory?

In this paper, we conduct a deep analysis at the intersection of LLM architecture and neuroscience. We first review current approaches for extending LLM memory, from Retrieval-Augmented Generation (RAG) with external databases to extended transformer attention windows and emerging episodic memory frameworks. Next, we explain the neuroscience of hippocampal-entorhinal cognitive maps and how they enable flexible memory-based planning through encoding of relationships and action–outcome linkages. We then explore how these neural principles can map onto computational designs – proposing memory-augmented LLM architectures that organize knowledge as a cognitive map for improved semantic retrieval, contextual adaptability, and long-term retention. We compare our hippocampal-inspired approach to recent large models (GPT-4.1, Claude 3.7, Gemini 2.5) in how they handle context and memory. We also incorporate insights from reinforcement learning (e.g. predictive representations and planning algorithms), neuro-symbolic reasoning, and multi-agent systems that complement our approach. Finally, we outline example architecture modifications and pseudocode prototypes demonstrating how such principles could be implemented in practice, and discuss future directions for memory-enhanced LLMs.

## Memory Architectures in Current LLMs

State-of-the-art LLMs primarily rely on two forms of memory: parametric memory (knowledge encoded in model weights through training) and contextual memory (information provided in the prompt context at inference). Parametric memory (analogous to semantic memory) gives models a vast base of world knowledge, but it is fixed after training and cannot capture specific instances or new events without fine-tuning. Contextual memory, on the other hand, is limited by the transformer's input length and thus serves as a sort of short-term working memory. This section surveys key techniques developed to extend LLMs' effective memory beyond these limitations:

### Retrieval-Augmented Generation (RAG)

RAG equips LLMs with an external knowledge repository and a retrieval mechanism. When given a query or task, the LLM first retrieves relevant text chunks from a database (often using vector embeddings for semantic search) and then conditions its generation on both the query and the retrieved texts. This approach allows long-term factual information to be pulled in as needed, rather than forcing the model to memorize everything in its parameters. Naïve RAG encodes documents into embedding vectors and finds nearest neighbors to the query embedding. One limitation is that vanilla embeddings capture surface-level similarity but not rich relational structure between pieces of knowledge. Recent variants therefore integrate more structure: Graph-RAG replaces the flat vector index with a graph database that explicitly encodes relationships between entities or facts. This can improve retrieval of contextually related information by traversing graph links (e.g. following a chain of connected concepts) rather than relying purely on vector similarity. However, even Graph-RAG typically includes only certain predefined relationship types and may not capture all context nuances. Overall, RAG-based methods provide explicit long-term storage and expansion of context by fetching relevant external information at query time, but designing the embedding or graph structure to fully mimic human-like recall remains challenging.

### External Vector Databases & Episodic Memory Buffers

In the context of LLM agents or continual conversations, it is common to maintain an external log of past interactions and facts (often as embedding vectors) that the model can consult. For example, a dialogue agent might store each conversation turn or each salient user fact in a vector database and use similarity search to retrieve them when contextually relevant (this is essentially a form of RAG specialized for dialogue history). Unlike static knowledge bases, such memory buffers can be dynamically updated with new experiences (forming an analog of episodic memory). Prior work has explored k-Nearest-Neighbor LMs that retrieve past training sentences to help predict next tokens (Khandelwal et al., 2020) as well as caching recent model outputs to reuse later. More recently, LLM-based interactive agents incorporate memory modules to record ongoing events: e.g. the "Generative Agents" framework (Park et al., 2023) logs an agent's observations and thoughts and uses embedding search to recall them, enabling believable long-term character behavior. These episodic memory frameworks often distinguish short-term context (immediately relevant info) from long-term storage (past episodes, summarized or in raw form). A key challenge is maintaining precision and relevance as the memory grows: storing every detail leads to noise and slow retrieval, whereas aggressive summarization risks losing important specifics. Approaches like score-based memory (keeping a running importance score for each memory) and semantic forgetting (pruning low-salience vectors over time) have been proposed to manage this trade-off.

### Extended Transformer Context (Long Attention Windows)

Another line of attack is purely architectural – improving the transformer's ability to handle long sequences. Research on efficient transformers has produced architectures that scale to tens or hundreds of thousands of tokens by using sparse attention, recurrence, or compressed representations (e.g. Longformer, BigBird, Transformer-XL, Reformer). Many of these were precursors to the large context capabilities now seen in production LLMs. 

The most recent advances in this area are substantial. GPT-4.1 (April 2025) supports a 1-million token context window, allowing it to process approximately 750,000 words at once. Gemini 2.5 Pro (2025) provides similar capacities. Claude 3.7 Sonnet, while not explicitly disclosing its context window size, likely falls in a similar range given competitive pressures. 

These mega-context models blur the line between retrieval and direct attention – essentially using the transformer itself to search for relevant info in a huge buffer. While impressive, there are real limitations: OpenAI's internal testing shows that GPT-4.1's accuracy falls from 84% with 8,000 tokens to just 50% with 1 million tokens, revealing diminishing returns from simply scaling context lengths. Additionally, extremely long contexts function as ephemeral memory: they vanish after a session and do not accumulate knowledge over time. In humans, working memory and attention are complemented by long-term memory consolidation; analogously, we may need LLM architectures that store information durably beyond a single context window.

### Emerging Memory Features in Commercial Systems

Recent developments in commercial LLMs show growing recognition of the importance of persistent memory. OpenAI has announced their intention for ChatGPT to "know you over your life" with upcoming Memory updates, suggesting movement toward persistent user models. ChatGPT has implemented "Projects" - a feature that allows grouping files and chats for personal use, with shared context between uploaded files and custom instructions.

Similarly, anthropomorphic AI assistants like Character.AI and Replika have implemented persistent memory systems to maintain consistent user relationships across sessions. These systems typically combine vector stores with summarization techniques to maintain a coherent user model over time.

However, these commercial implementations remain primarily focused on simple fact retrieval or preference modeling rather than the rich relational memory systems seen in human cognition. They generally lack the ability to flexibly navigate memory spaces, understand relationships between memories, or use memories for complex planning.

### Hybrid Memory Systems

The most promising direction is to combine the above strategies into a layered memory hierarchy, taking inspiration from cognitive systems. In fact, a recent study by Ahn (2025) introduced HEMA: a Hippocampus-Inspired Extended Memory Architecture for dialogues, explicitly structured after dual-memory theory. HEMA separates the LLM's memory into (1) a Compact Memory – essentially a continuously updated brief summary of the conversation (to maintain the global narrative in short form) – and (2) a Vector Memory – an episodic store of detailed past dialogue chunks as embeddings, which can be queried via cosine similarity. In each response, the system updates the compact summary (like a running gist, analogous to cortex storing a schema) and retrieves any relevant past chunk embeddings from the vector store to include in the prompt. This design allowed a 6-billion-parameter base model to sustain >300-turn conversations with only a 3.5K-token active prompt window. The dual memory dramatically improved factual recall (from 41% to 87% accuracy on long QA tasks) and coherence in storytelling (human ratings from 2.7 to 4.3 out of 5) compared to using just a summary or naive truncation. Such results validate that mimicking the brain's division of short- vs long-term memory can yield significant gains. Similarly, other work has explored "streaming" RAG agents that retrieve at every turn and even inject retrieved chunks into intermediate transformer layers (instead of only the input) to more tightly integrate external memory. These trends indicate a growing realization: LLMs need dedicated memory subsystems to handle long-term information, much as biological intelligences have the hippocampus and related structures alongside working memory in cortex.

## Hippocampal Cognitive Maps and Action–Outcome Learning

Decades of neuroscience research have converged on the idea that the hippocampus (in tandem with the entorhinal cortex) constructs a cognitive map of an organism's experiences. Originally proposed to explain spatial navigation (Tolman's "cognitive map", 1948; O'Keefe & Nadel's theory, 1978), this concept has broadened to encompass a more general relational map of different contexts, stimuli, and outcomes an agent encounters. In essence, the hippocampal-entorhinal system encodes relationships as geometric structures: neurons called place cells and grid cells fire in patterns that map to locations in physical or abstract spaces. This mapping allows flexible combination and comparison of memories.

A clear example is seen in how the brain navigates real space: as an animal explores an environment, hippocampal place cells fire for specific locations, while entorhinal grid cells provide a periodic lattice-like code for the 2D layout. Together they form a metric representation of space that the animal can use to plan routes, estimate distances, and remember locations of goals. Intriguingly, similar neural mechanisms apply to non-spatial tasks. Recent fMRI experiments in humans have shown that the hippocampus also encodes abstract relationships – effectively mapping conceptual spaces. Barnaveli et al. (2025) demonstrated that the entorhinal cortex exhibits hexagonal (six-fold) directional modulation of activity when people evaluate different action plans, analogous to the grid-cell hexagonal firing in spatial navigation. Participants in their study learned arbitrary associations between certain motor actions and abstract outcomes in a 2D "task space". When comparing these action–outcome pairs, neural activity reflected the geometry of the action-outcome space: entorhinal signals showed a periodic modulation every 60°, and hippocampal activity scaled continuously with the similarity of outcomes between action plans. In other words, the hippocampus was sensitive to how close two potential plans' outcomes were in the underlying map, and the entorhinal cortex encoded the orientation of the plan difference vector (hence the hexadirectional pattern). At the same time, frontal cortical areas (supplementary motor area) represented individual actions and had increased activation for overlapping action sequences. The hippocampus and frontal cortex interacted, with their connectivity varying according to action similarity, suggesting a cooperative role in evaluating and selecting plans.

These findings provide compelling evidence that cognitive maps support action selection. Rather than storing a giant lookup table of "action X → outcome Y" for every situation (which would be inefficient and rigid), the brain appears to organize actions and outcomes within a continuous relational space. In this space, similar outcomes cluster together and can be reached by similar action sequences, and entirely novel combinations can be interpolated. The entorhinal grid-like code provides a general coordinate system for this space, while the hippocampus links each specific event or experience (an action–outcome pair in context) to a location in the map. This enables powerful generalization: for example, if one action yields an outcome and another action yields a slightly different outcome, the map representation can reveal how those outcomes relate, helping predict a new action that might yield an intermediate outcome. Cognitive maps thus facilitate action–outcome prediction and imaginative planning ("if I do X, I expect Y, which is similar to what happened when I did X' last week"). They also naturally handle contextual variation – the same action might map to different outcome regions under different contexts, akin to how place cell maps can "remap" in a new environment.

Beyond action planning, hippocampal cognitive maps are known to represent conceptual relationships in various domains. For instance, studies have found that the hippocampus encodes relationships between non-spatial features (like odors or sounds) in a map-like manner, and entorhinal grid cells show hexagonal signals for abstract dimensions (e.g. social hierarchies or task schemas) much as they do for 2D space. The general picture is that the hippocampus provides a flexible representational substrate where any set of items that share underlying structured relationships (spatial, temporal, causal, etc.) can be mapped. By placing memories in this cognitive map, the brain can perform relational reasoning – computing distances, finding novel paths, grouping similar experiences – which is crucial for inference and decision-making.

Importantly, the hippocampus is also thought to serve as a fast learning module (for one-shot or rapid encoding of new episodes) that later consolidates knowledge to the cortex for long-term storage. This "complementary learning systems" theory aligns with the division between episodic memory (context-rich, instance-specific, dependent on hippocampus) and semantic memory (general, schema-based, stored in cortex). The Barnaveli et al. results challenge a strict separation of memory systems, showing the hippocampus is involved in goal-directed decision processes traditionally ascribed to procedural or semantic systems. It suggests memory and decision-making are deeply intertwined via the cognitive map: the brain organizes even its action policies in a memory structure that allows flexible recombination.

In summary, the hippocampal-entorhinal system provides a blueprint for efficient memory organization: it encodes experiences in a metric space where related experiences are near each other, enabling fast similarity searches and novel inferences. It rapidly stores new events (arbitrary action–outcome pairings) by embedding them into this map, and it interacts with decision-making circuits to guide choices based on past outcomes. These features – fast episodic encoding, relational mapping, and interaction with planning – are precisely what current LLMs lack when it comes to memory. In the next section, we discuss how to translate these neural principles into computational mechanisms to enhance LLM memory.

## From Neural Principles to Computational Memory Systems

The core ideas derived from hippocampal cognitive maps can inform several design principles for LLM memory: (1) Structured representations for stored knowledge (going beyond unstructured lists of text), (2) Relational retrieval that can flexibly connect related memories, not just exact matches, (3) Context-dependent encoding so that memories can be reinterpreted under new conditions, and (4) Fast learning of new information with gradual integration into the main knowledge base. We outline how each of these could be implemented:

### Memory as a Cognitive Map (Structured Semantic Space)

Instead of treating memory as a flat database of documents or a long text buffer, we can represent it as a learned high-dimensional space or graph where each memory item occupies a location determined by its content and relationships. Concretely, an LLM could be paired with a learned embedding function (analogous to entorhinal cortex/grid coding) that maps any item (e.g. a piece of text, a past conversation turn, an "event" consisting of an action and outcome) into a vector in a latent space. This space would be structured such that distance and direction have meaning – e.g., two memory vectors being close means those memories share semantic or contextual similarity. This is already partially true in vector embeddings used for retrieval, but a hippocampal approach would emphasize capturing multi-dimensional relations. We might augment the vector store with explicit connections (like a graph edge between memory A and memory B if B logically follows A, or if they occurred in the same context). This starts to resemble a knowledge graph, but it can be automatically constructed and continuous. By organizing knowledge into a cognitive map, an LLM's memory module could retrieve information not just by surface keywords, but by navigating the map: finding a memory that lies along the direction of the query vector, or retrieving a cluster of memories around a region relevant to the query. This addresses the limitation noted in naive RAG embeddings, which "do not encompass the rich set of relationships" needed. A cognitive map memory could encode rich relations implicitly through geometry (similar to how entorhinal grids encode multiple relational dimensions) or explicitly via stored links (similar to GraphRAG but learned). Recent work suggests LLMs could indeed benefit from such structures – the potential for LLMs to incorporate cognitive map structures has been highlighted as a promising direction.

### Relational and Contextual Retrieval

With a cognitive map memory, the retrieval process can go beyond simple nearest-neighbor text matching. We can implement algorithms to retrieve along relational paths. For example, if a user query asks a complex comparison question, the system might retrieve one memory item, then use its stored links or vector direction to hop to related items (analogous to graph traversal or following a trajectory in latent space). This is akin to mental associative recall ("this reminds me of X, which in turn reminds me of Y"). By using context as part of the query, we can achieve context-sensitive recall: in the brain, hippocampal recall is gated by the current context (e.g. your location or task sets what memories are relevant). Computationally, we could concatenate or fuse a context embedding with the query before searching memory. For instance, if the LLM is in a conversation about politics, the query embedding for memory search could be biased toward points in the map related to political discourse. Technically, one might train a context encoder that outputs a context vector, and define the memory retrieval similarity as a function of both content and context vectors (so that only memories from a matching context cluster are pulled). This would give contextual adaptability – the memory module retrieves different items for the same prompt depending on the situational context, much like our memories are context-dependent. Such an approach addresses the issue that in naive external memory, specific instances and context have to be provided as text keys to store/retrieve; a learned context-conditioned memory can generalize to implicit contextual cues.

### Action–Outcome Association and Predictive Memory

One of the hippocampal insights is the binding of actions with outcomes in a map for planning. To translate this, we consider LLMs in agentic roles (e.g. an LLM controlling tools or making decisions in a multi-step task). We can equip the LLM with a memory of state → action → outcome transitions. Every time the LLM (as an agent) takes an action (e.g. calls a tool, outputs a step in reasoning) and observes an outcome (result from the tool or the environment), it stores this as an experience in the cognitive map. Over time, the memory accumulates a network of states and outcomes reachable by various actions – essentially a learned world model. Given a new goal, the LLM can query this memory to simulate possible action sequences: this is analogous to how the brain's cognitive map is used for planning (the hippocampus might replay sequences or evaluate distances to goals). In AI terms, this relates to model-based reinforcement learning. A computational approach could be to use a successor representation (SR) or predictive encoding for memory entries – each memory could store not just the immediate outcome but a compressed representation of expected future outcomes from that state (successor features). This idea comes from RL theory (Dayan, 1993) and has been applied in neuroscience models to explain cognitive maps (Momennejad et al., 2017). By training the LLM's memory controller to predict outcomes of potential next actions (perhaps using a small internal simulation model), the system could choose the best action by "looking ahead" in memory. For instance, if an LLM-based agent has previously learned a sequence of API calls to accomplish a task, it can use the memory map to recognize a novel task as similar to a past one and retrieve that action sequence as a plan. This kind of action-conditioned memory retrieval would greatly improve LLMs' ability to carry knowledge forward into decision-making, rather than just static QA recall.

### Fast Memory Updates and Consolidation

In implementing a hippocampal-style memory, we want the ability to rapidly store new information (one-shot learning). This could be realized by having the LLM write to its external memory after each important event or at the end of each interaction. Writing could mean adding a new embedding (for the new knowledge or experience) to the vector database or memory graph. Such writes should be efficient (constant time, no retraining – an advantage of external memory). Over time, however, a large memory may become unwieldy, so we can borrow the concept of consolidation: periodically, some of the accumulated episodic memories can be distilled or integrated into the model's weights (analogous to transferring memory from hippocampus to cortex during sleep). In practice, this might involve fine-tuning the LLM on a summary of new high-value memories or adjusting its embedding space through contrastive learning so that it "bakes in" some frequently retrieved relationships. Designing the consolidation mechanism is beyond the scope of this paper, but we note it as a fruitful avenue to combine the plasticity of an external memory with the efficiency of internal (parametric) memory. Recent works on lifelong learning and knowledge editing in LLMs (e.g. Liu et al., 2023; Meng et al., 2023) provide potential tools for this, ensuring that as the LLM's memory grows, it remains consistent and does not forget earlier consolidated knowledge.

To illustrate how a simplified hippocampal-inspired memory system might work in an LLM, consider the following pseudocode for memory-augmented generation:

```python
# Pseudocode: Hippocampal-inspired memory retrieval in LLM
memory_db = VectorMemory()  # external memory store (vector DB or graph)

def encode(item_text):
    # Map an item (text or event) to a high-dimensional vector embedding
    return encoder_model(item_text)

def retrieve_relevant(memory_db, query_vec, context_vec=None, k=5):
    # If using context, combine it with query (e.g., elementwise or concatenation)
    if context_vec is not None:
        query_vec = combine(query_vec, context_vec)
    # Perform similarity search in the cognitive map space
    results = memory_db.nearest_neighbors(query_vec, top_k=k)
    return results  # returns list of memory items

def LLM_with_memory(input_text, context=None):
    # 1. Encode the input (query)
    q_vec = encode(input_text)
    ctx_vec = encode(context) if context else None
    # 2. Retrieve relevant memories
    retrieved_items = retrieve_relevant(memory_db, q_vec, ctx_vec, k=5)
    # 3. Integrate retrieved content into prompt (e.g., as citations or context passages)
    augmented_prompt = input_text + format_retrieved(retrieved_items)
    # 4. Generate LLM response using base model with augmented prompt
    output = base_LLM(augmented_prompt)
    # 5. Append the new interaction to memory (for future queries)
    memory_db.add( encode(context + " " + input_text + " => " + output) )
    return output
```

The benefits of a hippocampal-inspired memory system for LLMs include: (a) Semantic richness: because memories are stored with learned representations, the system can recall information by semantic similarity or analogy, not requiring exact prompts. (b) Adaptive context: the model can dynamically bring in facts from years ago or from early in a conversation when needed, without having them always in the prompt. (c) Long-term consistency: with a persistent memory, an LLM agent can develop stable knowledge about a user or a task over time (avoiding forgetting everything between sessions). (d) Efficient planning: memory entries can serve as building blocks for complex reasoning (the model can fetch a relevant solution example instead of computing from scratch). These align well with properties of human memory that make cognition flexible.

## Comparisons with Current LLMs (GPT-4.1, Claude 3.7, Gemini 2.5)

It is instructive to compare how our proposed neuroscience-inspired memory system differs from or complements the strategies used in today's top LLMs:

### GPT-4.1 (OpenAI)

GPT-4.1, released in April 2025, represents OpenAI's latest API model family. It supports up to 1 million tokens of context (approximately 750,000 words) and shows significant improvements in coding, instruction following, and long-context understanding compared to its predecessors. However, OpenAI's own testing reveals a significant limitation: the model's accuracy decreases from around 84% with 8,000 tokens to just 50% with 1 million tokens. This demonstrates that simply expanding context windows has diminishing returns.

Like previous models, GPT-4.1 doesn't have a native external memory API – it won't retain information between separate conversations without external engineering. Any long-term memory must be implemented by the user or developer storing data and re-feeding it. In effect, GPT-4.1's approach to memory remains "keep everything important in the prompt," with all the limitations that implies once you exceed optimal context lengths.

OpenAI has begun exploring persistent memory with features like "Projects" in ChatGPT and has mentioned plans to make ChatGPT "know you over your life" with upcoming Memory updates. However, these features still appear to use simple vector retrieval rather than the kind of rich relational memory system we propose.

A cognitive map module could enhance GPT-4.1 by enabling selective recall from outside the prompt – rather than trying to stuff a million tokens in the context (with declining accuracy), GPT-4.1 could retrieve just the most relevant information via a learned memory query, combining its strong reasoning capabilities with a more efficient long-term memory system.

### Claude 3.7 Sonnet (Anthropic)

Anthropic's Claude 3.7 Sonnet, the latest in their Claude 3 family as of May 2025, continues their focus on responsible AI with strong reasoning capabilities. While Anthropic hasn't publicly specified the exact context window size, it likely maintains competitive capacity with other leading models.

Like OpenAI's models, Claude relies primarily on its context window for memory, treating the entire prompt history as the episode to draw from. This works well for medium-length interactions but still faces the fundamental constraints of transformer attention mechanisms.

Claude doesn't yet feature a native external memory system for persistent storage across sessions, though Anthropic has discussed the importance of responsible information retention and context-aware interaction in their AI alignment research.

A hippocampal memory system would complement Claude's strong reasoning capabilities, allowing it to maintain consistent knowledge about users and tasks across multiple sessions without requiring all relevant information to be present in the prompt.

### Gemini 2.5 Pro (Google)

Google's Gemini 2.5 Pro, like GPT-4.1, features a 1-million token context window and is inherently multimodal, handling text, images, audio, and video. Google has demonstrated impressive capabilities in retrieving specific information from huge contexts with high accuracy.

Gemini's multimodal capabilities suggest it already maintains some form of cross-modal internal representation – potentially a step toward a richer "cognitive map" that spans multiple modalities. However, like its competitors, there's no indication that Gemini has a permanent memory store that outlives the current session.

For truly lifelong learning and interaction, even Gemini's million-token context isn't sufficient. Our hippocampal memory approach could enhance Gemini by providing structured, persistent storage of experiences and knowledge that can be flexibly accessed based on relevance to the current task or query.

### A Case for Hippocampal Memory Augmentation

In summary, GPT-4.1, Claude 3.7, and Gemini 2.5 represent a continuum of approaches to LLM memory, primarily relying on scaled-up context windows with increasingly sophisticated attention mechanisms. All of them still lack an architecturally distinct long-term memory module akin to what the hippocampus provides in biological systems. They manage context mainly via scaling, and any notion of past experiences must be reintroduced manually.

Our hippocampal-inspired approach is orthogonal and potentially synergistic: one could use a cognitive map memory alongside a large-context model to get the best of both. In fact, OpenAI's internal testing showing accuracy degradation with very long contexts suggests that simply expanding context might yield diminishing returns. This points to the need for new memory architectures.

A system with an effectively unbounded memory (backed by a database) and intelligent retrieval could surpass a fixed-context model in both scale and accuracy of recall, much like a person uses both active thinking and reference to written notes to handle vast information.

## Integrating Reinforcement Learning, Neuro-Symbolic Reasoning, and Multi-Agent Insights

Enhancing LLM memory by neural inspiration can be further guided by lessons from other areas: reinforcement learning (RL), neuro-symbolic AI, and multi-agent system design. Each of these domains has grappled with memory, planning, and knowledge representation in ways that intersect with our goals:

### Reinforcement Learning and Planning

In RL, an agent must remember and utilize past experiences (state, action, reward, next state) to improve future decisions. This has led to techniques such as experience replay, where past experiences are stored and replayed during training to stabilize learning – a direct analogue to the hippocampal replay of memories during sleep for consolidation. Moreover, the dichotomy of model-based vs model-free RL mirrors the difference between using a cognitive map to plan vs habitually reacting. Model-based RL algorithms explicitly learn a transition model (like a mental simulation of the environment) which they use for planning. The cognitive map in the hippocampus can be seen as a form of internal model – it encodes transitions (action-outcome relationships) and allows mental simulation (taking novel paths in the map). A computational parallel is the use of graph search or tree search over a learned state graph. For instance, an LLM with a memory graph of how knowledge points lead to each other could perform a search to answer a complex query (similar to reasoning over a knowledge graph). Additionally, the successor representation (SR) concept from RL, which we mentioned earlier, provides a mathematical way to encode future expected states. There has been recent work on using successor features in deep RL to enable transfer learning by capturing the "cognitive map" of tasks. An LLM agent could similarly use SR-like memory: each memory entry might include a vector that predicts which other memory entries are likely to follow if that memory is invoked. This is one way to integrate a notion of temporal or causal structure into the otherwise static embedding space. Reinforcement learning can also be used to train the memory retrieval policy – e.g., one can reward the LLM for fetching memories that lead to successful problem solving. This would align the memory usage with long-term utility, akin to how the brain's dopaminergic reward system might prioritize remembering events that led to positive outcomes. In short, RL offers algorithms for learning to navigate and update a memory graph optimally (value iteration on a cognitive map), which could greatly benefit a memory-augmented LLM that needs to decide what to recall and when.

### Neuro-Symbolic Reasoning

Another relevant thread is the integration of symbolic knowledge representations (like logic rules, graphs, databases) with neural networks. Neuro-symbolic systems aim to get

Evaluation Methodology and Metrics
To assess the effectiveness of our hippocampal-inspired memory approach compared to traditional LLM memory systems, we propose the following evaluation methodology:
1. Retrieval Accuracy and Relevance
	•	Needle-in-Haystack Test: Measure the model’s ability to find specific facts hidden in very long contexts (similar to Gemini’s evaluation, but across multiple sessions).
	•	Temporal Recall: Test recall of information from progressively older conversations (1 hour ago, 1 day ago, 1 week ago, 1 month ago).
	•	Context-Conditional Recall: Evaluate how well the system retrieves information that is relevant only in specific contexts.
	•	Precision@K: Measure the percentage of retrieved memories that are relevant to the current query.
	•	Recall@K: Measure the percentage of relevant memories that are successfully retrieved.
2. Long-Term Consistency
	•	Contradiction Detection: Track if the model contradicts previously stated information over extended interactions.
	•	Character Consistency: For narrative or role-playing applications, measure consistency of character traits over time.
	•	Factual Stability: Test whether the model maintains the same answers to factual questions over extended periods.
3. Planning and Action-Outcome Learning
	•	Multi-Step Task Completion: Evaluate the model’s ability to remember previously successful action sequences for similar tasks.
	•	Novel Path Planning: Measure how well the model can combine known action-outcome pairs to solve new problems.
	•	Tool Use Optimization: Test if the model learns to use tools more effectively over time based on past experiences.
4. Computational Efficiency
	•	Memory Usage: Compare storage requirements for different memory architectures (hippocampal map vs. raw text storage).
	•	Retrieval Speed: Measure retrieval time as memory size grows.
	•	Inference Cost: Compare computational cost between models with different memory architectures for the same tasks.
	•	Token Efficiency: Measure how many context tokens are required to maintain the same level of performance.
5. User Experience Metrics
	•	Response Relevance: Human ratings of how relevant the model’s responses are to the conversation history.
	•	Personalization: Measure how well the model adapts to individual users over time.
	•	Perceived Coherence: Human ratings of conversation naturalness and coherence over long interactions.
For these evaluations, we would construct benchmark datasets that specifically test long-term memory capabilities, including:
	1.	Multi-Session Dialogue Dataset: Simulated conversations spanning multiple sessions with critical information distributed across sessions.
	2.	Action-Outcome Corpus: Sequences of state-action-outcome triplets for testing the model’s ability to learn from past experiences.
	3.	Temporal Knowledge Base: Facts with timestamps to test recall of information from different time periods.
By comparing our hippocampal memory approach against traditional context window and RAG approaches across these metrics, we can quantify the benefits of structured memory representation and retrieval for different LLM applications.
Computational Overhead Analysis
While our proposed memory architecture offers significant improvements in long-term context management, it’s important to consider the computational costs compared to simpler approaches like expanded context windows:
Storage Requirements
	•	Vector Embeddings: Each memory item requires storing a high-dimensional vector (typically 768-1536 dimensions, ~3-6KB per memory).
	•	Relational Graphs: Storing relationship edges between memories adds overhead (typically 20-100 bytes per relationship).
	•	Metadata: Context information, timestamps, and tags add minimal overhead (~100-500 bytes per memory).
	•	Scaling Analysis: A system with 1 million memory items would require approximately 3-6GB for embeddings and 1-2GB for relationships and metadata.
Compared to raw text storage, our approach is more space-efficient for semantically equivalent content, as similar memories can be merged or linked rather than duplicated.
Retrieval Computation
	•	Vector Similarity Search: Modern vector databases can perform ANN (Approximate Nearest Neighbor) searches in sub-millisecond times even with millions of vectors.
	•	Graph Traversal: Following relationship edges adds computation time proportional to the number of hops (typically 1-10ms per hop).
	•	Context Integration: Combining context vectors with query vectors adds minimal overhead (~0.1ms).
	•	Scaling Behavior: Retrieval time scales logarithmically with the number of memories, not linearly, allowing efficient operation even with very large memory stores.
Inference Impact
	•	Prompt Construction: Adding retrieved memories to the prompt increases token count by approximately 5-15% compared to standard prompting.
	•	Additional Passes: Multiple memory retrievals during a single generation increase latency by ~50-200ms per retrieval.
	•	Embedding Generation: Creating embeddings for new memories adds approximately 10-50ms per memory item.
Efficiency Improvements
	•	Caching: Frequently accessed memories and their relationships can be cached for faster retrieval.
	•	Hierarchical Organization: Clustering memories into topics can reduce search space for most queries.
	•	Predictive Retrieval: The model can learn to predict which memories will be relevant, pre-fetching them before explicit queries.
	•	Memory Consolidation: Periodic summarization and pruning of similar memories reduces storage requirements and search space.
When compared to simply expanding context windows, our approach offers better computational efficiency for long-term interactions:
	1.	A 1-million token context window requires processing 1-2GB of text for every generation, while our approach typically retrieves only 1-5KB of relevant memories.
	2.	As OpenAI’s own testing shows, accuracy in very large contexts (1M tokens) drops significantly, while our targeted retrieval maintains high accuracy regardless of total memory size.
	3.	Our approach scales to effectively unlimited memory capacity without corresponding increases in inference time.
For a concrete comparison, consider a customer support agent that interacts with a user over months. Storing all past interactions in context would require millions of tokens, becoming computationally prohibitive. Our hippocampal approach could maintain the same level of personalization while operating within a 4-8K token context window, reducing inference cost by over 99% while maintaining or improving response quality.
Training Methodology
Implementing a hippocampal-inspired memory system requires training several components to work in concert. Here we outline a comprehensive training methodology:
1. Memory Encoder Training
The memory encoder (analogous to the entorhinal cortex) maps text to vectors in a structured semantic space:
	1.	Contrastive Learning: Train the encoder using contrastive objectives (like SimCSE or CLIP) where semantically related texts are pulled together and unrelated texts are pushed apart.
	2.	Triplet Loss Training: Use triplets of (anchor, positive, negative) text samples to structure the embedding space.
	3.	Multi-dimensional Scaling: Train the encoder to preserve multiple types of relationships (temporal, causal, categorical) simultaneously in the embedding space.
	4.	Context-Conditional Embedding: Fine-tune with pairs of (text, context) inputs to learn how context modifies the meaning of text.
2. Relation Graph Construction
Train models to predict relationships between memory items:
	1.	Temporal Sequencing: Train a model to predict chronological ordering between memories.
	2.	Causal Relationship Detection: Train a model to detect when one memory describes a cause and another describes an effect.
	3.	Entity Resolution: Train a model to detect when different memories refer to the same entities.
	4.	Graph Structure Learning: Apply graph neural networks to learn optimal connectivity patterns between memories.
3. Retrieval Policy Training
Train the system to retrieve the most relevant memories for a given query:
	1.	Supervised Retrieval: Using pairs of (query, relevant memories) examples from human annotations.
	2.	Reinforcement Learning: Reward the retrieval mechanism when it provides memories that improve the LLM’s response quality.
	3.	Counterfactual Learning: Train on examples where different sets of memories lead to different responses, learning which memories would have been most useful.
	4.	Multi-hop Reasoning: Train explicit multi-hop retrieval strategies for complex queries requiring following chains of relationships.
4. Memory Consolidation Training
Train mechanisms to maintain memory efficiency over time:
	1.	Importance Prediction: Train a model to predict which memories will be useful in the future.
	2.	Clustering and Summarization: Train models to identify clusters of similar memories and create representative summaries.
	3.	Information Preservation: Ensure consolidation processes preserve critical information while reducing redundancy.
	4.	Active Forgetting: Train explicit forgetting mechanisms that selectively remove low-value memories.
5. End-to-End Training
Integrate the memory system with the base LLM:
	1.	Memory-Augmented Prompting: Fine-tune the LLM to effectively use information from retrieved memories.
	2.	Memory Writing: Train the LLM to identify important information that should be stored in memory.
	3.	Retrieval Requests: Train the LLM to explicitly request memory retrievals when needed during generation.
	4.	Memory Integrity: Train the system to verify and correct inconsistent memories.
Training Data Sources
	1.	Dialogue Datasets: Multi-turn conversations that require recalling previous interactions.
	2.	QA with Context: Questions that require referring to specific documents or facts.
	3.	Procedural Tasks: Step-by-step instructions where remembering past steps is critical.
	4.	Narrative Comprehension: Stories with characters and events that must be tracked consistently.
	5.	Synthetic Data Generation: Use existing LLMs to generate diverse memory-intensive scenarios.
Curriculum Learning Approach
We propose a staged training curriculum:
	1.	Start with simple memory tasks (direct fact retrieval with short context windows).
	2.	Progress to temporal recall (retrieving information from increasingly distant past contexts).
	3.	Advance to relational reasoning (retrieving connected memories across relationship edges).
	4.	Culminate with planning tasks (using memories of past actions to inform new decisions).
This curriculum ensures each component develops foundational capabilities before being integrated into the more complex complete system.
Ethical Considerations
Implementing persistent memory systems for LLMs raises important ethical considerations that must be addressed:
Privacy and Data Retention
	•	User Consent: Systems must obtain explicit consent from users before storing their information in long-term memory.
	•	Right to Be Forgotten: Users should have the ability to request deletion of specific memories or all their personal data.
	•	Memory Boundaries: Clear policies must define what information is appropriate to remember and what should be ephemeral.
	•	Private vs. Shared Memories: Systems should distinguish between user-specific memories and knowledge that can be generalized across users.
Memory Accuracy and Bias
	•	False Memories: Systems might encode incorrect information, requiring mechanisms to verify and correct memories.
	•	Selective Memory: The choice of what to remember can introduce or amplify biases in the system’s knowledge.
	•	Memory Attribution: Systems should maintain provenance information for memories to evaluate their reliability.
	•	Memory Auditing: Regular audits should be conducted to identify and address biased or harmful patterns in stored memories.
Security Considerations
	•	Memory Access Control: Different types of memories may require different levels of access control.
	•	Memory Poisoning: Adversaries might attempt to intentionally introduce false or harmful information into the memory system.
	•	Inference Attacks: Analyzing memory retrieval patterns could potentially reveal sensitive information about users.
	•	Memory Encryption: Sensitive personal memories should be encrypted to prevent unauthorized access.
Social and Psychological Impact
	•	Dependency Risks: Users might become dependent on AI memory systems, potentially affecting their own memory skills.
	•	Relationship Boundaries: Memory-enabled AI systems may create illusions of genuine relationships that could affect vulnerable users.
	•	Trust Calibration: Users must understand the limitations of AI memory systems to maintain appropriate trust levels.
	•	Identity Implications: Systems that remember user preferences and history might influence users’ sense of identity over time.
Implementation Guidelines
To address these concerns, we recommend:
	1.	Transparent Memory Policies: Clearly communicate what information is stored, for how long, and how it will be used.
	2.	Granular Control: Provide users with easy-to-use controls for reviewing, editing, and deleting specific memories.
	3.	Memory Tiering: Implement different retention policies for different types of information (e.g., factual vs. personal).
	4.	Regular Auditing: Conduct systematic reviews of memory contents to identify and address potential issues.
	5.	Responsible Deployment: Initially limit deployment to applications with lower privacy and safety risks while refining safety measures.
By thoughtfully addressing these ethical considerations, we can develop memory-enhanced LLMs that respect user privacy and autonomy while providing the benefits of long-term personalization and consistency.
Conclusion
We have explored how insights from the neuroscience of memory – particularly the role of hippocampal cognitive maps – can inspire the next generation of memory systems in LLMs. By reviewing current LLM memory architectures, we found that while techniques like RAG and large context windows extend an LLM’s recall to an extent, they fall short of the flexibility, efficiency, and longevity of biological memory. The hippocampal-entorhinal system offers a compelling model: it encodes experiences in a structured form that enables fast relational queries and supports planning through associative links between actions and outcomes. Translating these principles, we proposed an LLM architecture with a dedicated memory module that stores information in a learned cognitive map, enabling semantic and context-dependent retrieval, and even prediction of outcomes for decision support. We discussed how such a system can

### Neuro-Symbolic Reasoning

Another relevant thread is the integration of symbolic knowledge representations (like logic rules, graphs, databases) with neural networks. Neuro-symbolic systems aim to get the best of both worlds: the robustness and pattern recognition of neural models with the precision and interpretability of symbolic structures. Our hippocampal memory proposal already edges into this territory by representing memory as a sort of graph or at least a set of vectors with relational meaning. We can deepen the neuro-symbolic connection by considering using knowledge graphs as part of the LLM's memory. For example, factual knowledge (like a medical knowledge base or a family tree of characters in a story) could be stored in a graph database. The LLM, when needing a fact, could translate its query into a graph query (this might involve a module that maps natural language questions to SPARQL or Cypher queries, for instance), retrieve the precise symbolic answer, and then incorporate it into the reasoning. This approach has been explored in question-answering systems where the model consults a knowledge base for exact answers (ensuring factual accuracy). The limitation historically was that hard symbols are not differentiable, but with large LLMs one can use a soft interface (the model generates the query textually, and the result is fed back as text). Neuro-symbolic memory could also help with long-term consistency: for instance, maintaining a symbolic store of all the facts the model has stated about a fictional world in a storytelling application, to avoid contradictions. The model's neural part (like the transformer) may forget a detail from chapter 1 by chapter 10, but the symbolic memory can remind it of that detail when needed. This is analogous to how an author might use notes or an index to keep track of a complex plot – the hippocampus itself might play a role in indexing such details in our brain's "story memory". We note that Graph-RAG methods already incorporate some symbolic structure, but a hippocampal approach would manage the symbolic knowledge in a more autonomous and learned way. Additionally, logic-based reasoning (like solving a puzzle or mathematical proof) can strain an LLM's short-term memory, but if the LLM could offload certain facts or sub-results to a symbolic memory (or an external solver) and retrieve them correctly later, it could handle deeper chains of reasoning. In essence, neuro-symbolic insights encourage us to use structured memory for what it's best at (exact recall, enforcing constraints) and use neural processing for what it's best at (flexible inference, understanding language). A hippocampal memory could store structured facts (like a table of known true statements) as well as unstructured experiences, and an LLM agent could reason by interleaving neural generation with symbolic memory lookups. This hybrid approach has shown promise in systems like Program-aided Language Models (which write and execute code to aid reasoning) – writing and executing code is a form of querying an external symbolic system (the computer). Likewise, querying a memory database is a simpler form of that idea.

### Multi-Agent Systems and Memory Sharing

In multi-agent environments, agents often need to share knowledge or coordinate, which brings interesting perspectives to memory design. If we consider a single intelligent agent, it might actually be decomposed into multiple sub-agents or modules – a concept reminiscent of Marvin Minsky's "Society of Mind". For example, we could have a Memory Agent whose job is to observe interactions and store/retrieve information, and a Dialogue/Reasoning Agent that converses with the user and solves problems, asking the Memory Agent for help when needed. This kind of architecture has been explored in some LLM-based systems. In fact, the "Generative Agents" work implemented something along these lines: each simulated character had mechanisms for memory (storing observations), reflection (to synthesize and compress memories), and planning, all orchestrated around an LLM. One could think of these mechanisms as distinct agents that communicate (even if implemented within one process). The benefit of a multi-agent (or modular) approach is specialization – the memory component can be tuned for search and update efficiency, while the language component is tuned for fluency and reasoning. They communicate via a shared language or protocol. This separation also mirrors brain systems: the hippocampus (memory index) and the cortex (higher-level reasoning and perception) are separate but exchange information. Multi-agent frameworks also allow for distributed memory: for instance, different agents might remember different aspects of the world (like a division of knowledge by topic), and a "manager" agent directs queries to the right specialist. This is analogous to how human organizations or even the internet works – no single unit holds all knowledge, but a network of specialists can be queried. For LLMs, this could mean having a factual memory agent (that uses a Wikipedia-like knowledge base), a personal memory agent (that remembers user-specific context), etc. In a interactive setting, one agent could observe the environment continuously and log events, while another only accesses those logs on request. Some recent multi-agent simulations had agents writing summaries to a "blackboard" that others can read. That blackboard is essentially a shared memory – an external store all agents can query. In LLM terms, this could be a shared vector database that multiple LLM instances use.

Furthermore, multi-agent interactions sometimes produce emergent behavior that a single model may struggle with. For example, two agents can debate a question, with each one recalling different evidence, leading to a better answer. This can reduce mistakes and provide a form of checking memory: one agent's memory retrieval can be verified by another. It also resonates with the idea of memory reconsolidation – where recall of a memory presents an opportunity to verify and possibly correct that memory (which in a social context could be like one person correcting another's recollection). While a full multi-agent design is beyond our scope, we foresee that a modular LLM system with a dedicated memory module (or agent) and other utility agents (for tools, calculations, etc.) will behave more robustly than a monolithic model. This is supported by the improvements seen in the generative agents' long-term consistency and other LLM agent frameworks that explicitly include memory and planning components.

Incorporating these insights, a hippocampal-inspired LLM memory system could be trained using RL (to reinforce useful memory retrieval behaviors), could store some knowledge in symbolic forms (for exact recall), and could even be conceptualized as a distinct sub-agent in a larger AI system. All these are complementary to the core idea of a cognitive map: they offer practical means to implement and enhance it.

## Prototype Architecture and Implementation Examples

Bringing all the pieces together, we propose an LLM architecture augmented with a hippocampal memory module as illustrated conceptually in Figure 1 (see below). The design consists of two main components:

1. **Core LLM (Transformer Decoder)**: This is the primary language model that generates responses. It still has a limited context window (e.g. 4K or 8K tokens) for immediate working memory. It handles grammar, reasoning, and interaction with the user. It is pre-trained on large text corpora (giving it a wealth of semantic memory in its weights) but is extended with additional inputs/outputs to interface with the memory module.

2. **Memory Module (Cognitive Map Database + Controller)**: This module maintains a persistent store of memory items, each encoded as vectors (and possibly links) in a high-dimensional space. It also includes a controller that can perform operations like store, retrieve, update, and consolidate. The memory items could be: chunks of text from past conversations, facts the user has provided, intermediate reasoning steps the model generated (chain-of-thought logs), outcomes of actions or tool invocations, and so on – each tagged with context meta-data (time, topic, origin). The memory controller would use one or more learned embedding models to encode queries and memories (these could be transformer encoders fine-tuned for this purpose).

![Figure 1: Hippocampal-Inspired LLM Architecture](https://i.imgur.com/placeholder.png)

The interaction between these components proceeds in a loop during a conversation or task:

- When a new user query comes in, the LLM generates an embedding query (this could be a hidden state from a special [MEM] token at the end of the input). This query is sent to the Memory Module.
- The Memory Module finds relevant entries (using techniques described earlier: similarity search, graph traversal, etc.) and returns a set of retrieved items (as text snippets or aggregated info).
- The LLM then receives these retrieved results as additional context (for example, they could be prepended to the input with special tokens indicating they are memory). Alternatively, the LLM might incorporate them through a dedicated attention head that attends to memory embeddings (a more architectural merge, as explored in some "memory transformers" like the DNC which had separate memory read heads).
- The LLM uses both the user query and memory to generate an answer. Importantly, it can also decide to query memory multiple times or not at all, depending on the task. For instance, if the question is trivial and answerable from short-term info, maybe no retrieval is needed. But if the question is complex, the LLM might make an initial query, get some info, then later in the response generation realize it needs another fact and issue another query (this could be done by a feedback mechanism or by planning out multiple retrievals via a chain-of-thought).
- After responding, the system takes the whole interaction (or any new facts) and encodes it into the memory. This could be immediate or scheduled (perhaps trivial queries are not stored, but important events are).
- Optionally, a consolidation routine runs at intervals to summarize recent dialogues into a gist (analogous to HEMA's compact memory) and prune or compress the memory graph.

As a concrete example, imagine an LLM assistant enhanced with this memory. The user says: "I just moved to a new city and I'm looking for a good Italian restaurant. Last week you recommended a few places – can you remind me which one had the wood-fired oven and live music?" A vanilla LLM with no memory might not recall last week's conversation (unless the user re-pasted it). But our system would encode the prior chat about restaurants as memory. When this query comes, the memory module, given the cue "Italian restaurant + wood-fired oven + live music", would retrieve the specific restaurant info from last week (since those keywords/features map closely in the vector space to that memory entry). It returns the result, say: "La Trattoria – known for its wood-fired pizzas and live jazz on weekends". The core LLM receives this and can answer: "The restaurant I recommended with a wood-fired oven and live music was La Trattoria. It has a great atmosphere with jazz performances on weekends." This kind of personalized recall is only possible with an external memory (GPT-4.1 or Claude 3.7 wouldn't remember a conversation from a week ago unless it was given again in prompt).

For an implementation perspective, much of the infrastructure for this already exists in various forms:

- Vector databases (like FAISS, Milvus, Pinecone) can handle millions of embeddings and perform fast similarity search. Integrating one with an LLM is straightforward with current APIs.
- Prompt engineering can incorporate retrieved text via template (many RAG implementations simply insert a section like "Relevant info:" followed by retrieved passages before answering).
- There are libraries and frameworks (LangChain, LlamaIndex) that abstract some of this memory retrieval process for LLM applications. These could be extended with more advanced retrieval logic (like graph-based retrieval or multiple hops).
- Training the LLM to use the memory might involve fine-tuning it with examples where it's shown how retrieved info should be used. Alternatively, one can design the system such that the LLM's generation naturally benefits from the extra info (e.g., if the memory text is clearly relevant, an off-the-shelf model will usually incorporate it into the answer if instructed).
- Ensuring the LLM trusts the memory and avoids hallucinating when memory is present is important. This can be aided by copy-mechanism style training or by having the LLM cite the memory sources (as some QA systems do).

One could also implement parts of the memory module as differentiable components attached to the transformer – for instance, adding a trainable key-value memory as an additional layer. Past research like the Differentiable Neural Computer (DNC) and Neural Turing Machines showed that neural networks can learn to write to and read from an external memory matrix. Recent transformer variants sometimes include trainable memory tokens or an additional recurrent memory. However, those are usually limited in size. A full cognitive map memory might be too large to keep differentiable end-to-end (millions of entries), hence a non-differentiable retrieval (like an API call to a DB) with possibly some learning (via RL or supervised signals to tune the retrieval embedding) might be more practical.

### Code Implementation Examples

Below we provide two detailed code examples of core components in our hippocampal-inspired memory system:

#### Cognitive Map Memory Implementation

```python
class CognitiveMapMemory:
    def __init__(self, embedding_dim=768, similarity_threshold=0.75):
        self.vector_store = VectorDatabase(dim=embedding_dim)  
        self.relation_graph = RelationGraph()  
        self.encoder = TransformerEncoder("memory-encoder-v1")
        self.similarity_threshold = similarity_threshold
        
    def encode(self, text, context=None):
        if context:
            # Concatenate text with context for contextual embedding
            input_text = f"Context: {context} Content: {text}"
        else:
            input_text = text
        return self.encoder.encode(input_text)
        
    def store(self, memory_item, context=None):
        # Create memory embedding considering context
        embedding = self.encode(memory_item["text"], context)
        memory_id = self.vector_store.add(embedding, memory_item)
        
        # Find semantically similar memories
        similar_memories = self.vector_store.find_similar(
            embedding, 
            k=5, 
            threshold=self.similarity_threshold
        )
        
        # Create relational connections based on similarity and temporal order
        for similar_id in similar_memories:
            similar_item = self.vector_store.get_item(similar_id)
            relation_type = self.determine_relation(memory_item, similar_item)
            self.relation_graph.add_edge(memory_id, similar_id, relation_type)
            
        return memory_id
            
    def determine_relation(self, source_item, target_item):
        # Determine relation type based on item properties
        if source_item.get("timestamp") and target_item.get("timestamp"):
            if source_item["timestamp"] < target_item["timestamp"]:
                return "precedes"
            else:
                return "follows"
        
        if source_item.get("type") == "action" and target_item.get("type") == "outcome":
            return "causes"
            
        # Default semantic similarity relation
        return "similar_to"
            
    def retrieve(self, query, context=None, retrieval_type="similarity", max_results=10):
        query_embedding = self.encode(query, context)
        
        if retrieval_type == "similarity":
            # Direct similarity search
            results = self.vector_store.nearest_neighbors(query_embedding, k=max_results)
            return [self.vector_store.get_item(id) for id in results]
            
        elif retrieval_type == "relational":
            # Graph-based retrieval (following connections)
            initial_matches = self.vector_store.nearest_neighbors(query_embedding, k=3)
            expanded_results = self.relation_graph.expand_from_nodes(
                initial_matches, 
                max_hops=2, 
                max_results=max_results
            )
            return [self.vector_store.get_item(id) for id in expanded_results]
            
        elif retrieval_type == "temporal":
            # Time-based retrieval (recent memories first)
            initial_matches = self.vector_store.nearest_neighbors(query_embedding, k=5)
            return sorted(
                [self.vector_store.get_item(id) for id in initial_matches],
                key=lambda x: x.get("timestamp", 0),
                reverse=True
            )[:max_results]
```

#### Action-Outcome Planning System

```python
class ActionOutcomeMap:
    def __init__(self, memory_system, prediction_horizon=5):
        self.memory = memory_system
        self.successor_model = SuccessorRepresentationModel(
            embedding_dim=768,
            hidden_dim=1024,
            prediction_horizon=prediction_horizon
        )
        self.action_value_cache = {}  # Cache for frequently accessed action values
        
    def record_action_outcome(self, state, action, outcome, next_state, reward=None):
        # Create structured memory item
        memory_item = {
            "type": "action_outcome",
            "state": state,
            "action": action,
            "outcome": outcome,
            "next_state": next_state,
            "reward": reward,
            "timestamp": time.time()
        }
        
        # Store in cognitive map memory
        memory_id = self.memory.store(memory_item, context=state)
        
        # Update successor prediction model
        state_embedding = self.memory.encode(state)
        next_state_embedding = self.memory.encode(next_state)
        action_embedding = self.memory.encode(action)
        
        self.successor_model.update(
            state_embedding, 
            action_embedding,
            next_state_embedding,
            reward
        )
        
        # Invalidate cached values that depend on this state
        self._invalidate_related_cache(state)
        
        return memory_id
        
    def _invalidate_related_cache(self, state):
        # Remove cached values related to this state
        state_embedding = self.memory.encode(state)
        keys_to_remove = []
        
        for key in self.action_value_cache:
            cache_state, _ = key
            if vector_similarity(cache_state, state_embedding) > 0.9:
                keys_to_remove.append(key)
                
        for key in keys_to_remove:
            del self.action_value_cache[key]
        
    def plan_action(self, current_state, goal_state, max_candidates=10, exploration_factor=0.1):
        """
        Find the best action to reach goal_state from current_state using memory
        and predictive modeling
        """
        # Encode states
        current_state_embedding = self.memory.encode(current_state)
        goal_state_embedding = self.memory.encode(goal_state)
        
        # Retrieve similar past states from memory
        similar_state_experiences = self.memory.retrieve(
            current_state,
            retrieval_type="similarity",
            max_results=max_candidates
        )
        
        # Evaluate potential actions based on predicted outcomes
        potential_actions = []
        for exp in similar_state_experiences:
            if "action" not in exp or "next_state" not in exp:
                continue
                
            action = exp["action"]
            action_embedding = self.memory.encode(action)
            
            # Check cache first
            cache_key = (tuple(current_state_embedding), tuple(action_embedding))
            if cache_key in self.action_value_cache:
                predicted_outcome, value = self.action_value_cache[cache_key]
            else:
                # Predict outcome using successor model
                predicted_outcome = self.successor_model.predict(
                    current_state_embedding,
                    action_embedding
                )
                
                # Calculate value (similarity to goal)
                value = vector_similarity(predicted_outcome, goal_state_embedding)
                
                # Cache the result
                self.action_value_cache[cache_key] = (predicted_outcome, value)
            
            # Add exploration bonus for less-frequently used actions
            exploration_bonus = exploration_factor * random.random()
            adjusted_value = value + exploration_bonus
            
            potential_actions.append((action, adjusted_value, value))
        
        # Return best action or None if no viable actions found
        if not potential_actions:
            return None
            
        # Sort by adjusted value (with exploration), but return original value too
        potential_actions.sort(key=lambda x: x[1], reverse=True)
        best_action, _, original_value = potential_actions[0]
        
        return {
            "action": best_action,
            "confidence": original_value,
            "alternatives": [a[0] for a in potential_actions[1:3]]  # Next best options
        }
```

## Evaluation Methodology and Metrics

To assess the effectiveness of our hippocampal-inspired memory approach compared to traditional LLM memory systems, we propose the following evaluation methodology:

### 1. Retrieval Accuracy and Relevance

- **Needle-in-Haystack Test**: Measure the model's ability to find specific facts hidden in very long contexts (similar to Gemini's evaluation, but across multiple sessions).
- **Temporal Recall**: Test recall of information from progressively older conversations (1 hour ago, 1 day ago, 1 week ago, 1 month ago).
- **Context-Conditional Recall**: Evaluate how well the system retrieves information that is relevant only in specific contexts.
- **Precision@K**: Measure the percentage of retrieved memories that are relevant to the current query.
- **Recall@K**: Measure the percentage of relevant memories that are successfully retrieved.

### 2. Long-Term Consistency

- **Contradiction Detection**: Track if the model contradicts previously stated information over extended interactions.
- **Character Consistency**: For narrative or role-playing applications, measure consistency of character traits over time.
- **Factual Stability**: Test whether the model maintains the same answers to factual questions over extended periods.

### 3. Planning and Action-Outcome Learning

- **Multi-Step Task Completion**: Evaluate the model's ability to remember previously successful action sequences for similar tasks.
- **Novel Path Planning**: Measure how well the model can combine known action-outcome pairs to solve new problems.
- **Tool Use Optimization**: Test if the model learns to use tools more effectively over time based on past experiences.

### 4. Computational Efficiency

- **Memory Usage**: Compare storage requirements for different memory architectures (hippocampal map vs. raw text storage).
- **Retrieval Speed**: Measure retrieval time as memory size grows.
- **Inference Cost**: Compare computational cost between models with different memory architectures for the same tasks.
- **Token Efficiency**: Measure how many context tokens are required to maintain the same level of performance.

### 5. User Experience Metrics

- **Response Relevance**: Human ratings of how relevant the model's responses are to the conversation history.
- **Personalization**: Measure how well the model adapts to individual users over time.
- **Perceived Coherence**: Human ratings of conversation naturalness and coherence over long interactions.

For these evaluations, we would construct benchmark datasets that specifically test long-term memory capabilities, including:

1. **Multi-Session Dialogue Dataset**: Simulated conversations spanning multiple sessions with critical information distributed across sessions.
2. **Action-Outcome Corpus**: Sequences of state-action-outcome triplets for testing the model's ability to learn from past experiences.
3. **Temporal Knowledge Base**: Facts with timestamps to test recall of information from different time periods.

By comparing our hippocampal memory approach against traditional context window and RAG approaches across these metrics, we can quantify the benefits of structured memory representation and retrieval for different LLM applications.

## Computational Overhead Analysis

While our proposed memory architecture offers significant improvements in long-term context management, it's important to consider the computational costs compared to simpler approaches like expanded context windows:

### Storage Requirements

- **Vector Embeddings**: Each memory item requires storing a high-dimensional vector (typically 768-1536 dimensions, ~3-6KB per memory).
- **Relational Graphs**: Storing relationship edges between memories adds overhead (typically 20-100 bytes per relationship).
- **Metadata**: Context information, timestamps, and tags add minimal overhead (~100-500 bytes per memory).
- **Scaling Analysis**: A system with 1 million memory items would require approximately 3-6GB for embeddings and 1-2GB for relationships and metadata.

Compared to raw text storage, our approach is more space-efficient for semantically equivalent content, as similar memories can be merged or linked rather than duplicated.

### Retrieval Computation

- **Vector Similarity Search**: Modern vector databases can perform ANN (Approximate Nearest Neighbor) searches in sub-millisecond times even with millions of vectors.
- **Graph Traversal**: Following relationship edges adds computation time proportional to the number of hops (typically 1-10ms per hop).
- **Context Integration**: Combining context vectors with query vectors adds minimal overhead (~0.1ms).
- **Scaling Behavior**: Retrieval time scales logarithmically with the number of memories, not linearly, allowing efficient operation even with very large memory stores.

### Inference Impact

- **Prompt Construction**: Adding retrieved memories to the prompt increases token count by approximately 5-15% compared to standard prompting.
- **Additional Passes**: Multiple memory retrievals during a single generation increase latency by ~50-200ms per retrieval.
- **Embedding Generation**: Creating embeddings for new memories adds approximately 10-50ms per memory item.

### Efficiency Improvements

- **Caching**: Frequently accessed memories and their relationships can be cached for faster retrieval.
- **Hierarchical Organization**: Clustering memories into topics can reduce search space for most queries.
- **Predictive Retrieval**: The model can learn to predict which memories will be relevant, pre-fetching them before explicit queries.
- **Memory Consolidation**: Periodic summarization and pruning of similar memories reduces storage requirements and search space.

When compared to simply expanding context windows, our approach offers better computational efficiency for long-term interactions:

1. A 1-million token context window requires processing 1-2GB of text for every generation, while our approach typically retrieves only 1-5KB of relevant memories.
2. As OpenAI's own testing shows, accuracy in very large contexts (1M tokens) drops significantly, while our targeted retrieval maintains high accuracy regardless of total memory size.
3. Our approach scales to effectively unlimited memory capacity without corresponding increases in inference time.

For a concrete comparison, consider a customer support agent that interacts with a user over months. Storing all past interactions in context would require millions of tokens, becoming computationally prohibitive. Our hippocampal approach could maintain the same level of personalization while operating within a 4-8K token context window, reducing inference cost by over 99% while maintaining or improving response quality.

## Training Methodology

Implementing a hippocampal-inspired memory system requires training several components to work in concert. Here we outline a comprehensive training methodology:

### 1. Memory Encoder Training

The memory encoder (analogous to the entorhinal cortex) maps text to vectors in a structured semantic space:

1. **Contrastive Learning**: Train the encoder using contrastive objectives (like SimCSE or CLIP) where semantically related texts are pulled together and unrelated texts are pushed apart.
2. **Triplet Loss Training**: Use triplets of (anchor, positive, negative) text samples to structure the embedding space.
3. **Multi-dimensional Scaling**: Train the encoder to preserve multiple types of relationships (temporal, causal, categorical) simultaneously in the embedding space.
4. **Context-Conditional Embedding**: Fine-tune with pairs of (text, context) inputs to learn how context modifies the meaning of text.

### 2. Relation Graph Construction

Train models to predict relationships between memory items:

1. **Temporal Sequencing**: Train a model to predict chronological ordering between memories.
2. **Causal Relationship Detection**: Train a model to detect when one memory describes a cause and another describes an effect.
3. **Entity Resolution**: Train a model to detect when different memories refer to the same entities.
4. **Graph Structure Learning**: Apply graph neural networks to learn optimal connectivity patterns between memories.

### 3. Retrieval Policy Training

Train the system to retrieve the most relevant memories for a given query:

1. **Supervised Retrieval**: Using pairs of (query, relevant memories) examples from human annotations.
2. **Reinforcement Learning**: Reward the retrieval mechanism when it provides memories that improve the LLM's response quality.
3. **Counterfactual Learning**: Train on examples where different sets of memories lead to different responses, learning which memories would have been most useful.
4. **Multi-hop Reasoning**: Train explicit multi-hop retrieval strategies for complex queries requiring following chains of relationships.

### 4. Memory Consolidation Training

Train mechanisms to maintain memory efficiency over time:

1. **Importance Prediction**: Train a model to predict which memories will be useful in the future.
2. **Clustering and Summarization**: Train models to identify clusters of similar memories and create representative summaries.
3. **Information Preservation**: Ensure consolidation processes preserve critical information while reducing redundancy.
4. **Active Forgetting**: Train explicit forgetting mechanisms that selectively remove low-value memories.

### 5. End-to-End Training

Integrate the memory system with the base LLM:

1. **Memory-Augmented Prompting**: Fine-tune the LLM to effectively use information from retrieved memories.
2. **Memory Writing**: Train the LLM to identify important information that should be stored in memory.
3. **Retrieval Requests**: Train the LLM to explicitly request memory retrievals when needed during generation.
4. **Memory Integrity**: Train the system to verify and correct inconsistent memories.

### Training Data Sources

1. **Dialogue Datasets**: Multi-turn conversations that require recalling previous interactions.
2. **QA with Context**: Questions that require referring to specific documents or facts.
3. **Procedural Tasks**: Step-by-step instructions where remembering past steps is critical.
4. **Narrative Comprehension**: Stories with characters and events that must be tracked consistently.
5. **Synthetic Data Generation**: Use existing LLMs to generate diverse memory-intensive scenarios.

### Curriculum Learning Approach

We propose a staged training curriculum:

1. Start with simple memory tasks (direct fact retrieval with short context windows).
2. Progress to temporal recall (retrieving information from increasingly distant past contexts).
3. Advance to relational reasoning (retrieving connected memories across relationship edges).
4. Culminate with planning tasks (using memories of past actions to inform new decisions).

This curriculum ensures each component develops foundational capabilities before being integrated into the more complex complete system.

## Ethical Considerations

Implementing persistent memory systems for LLMs raises important ethical considerations that must be addressed:

### Privacy and Data Retention

- **User Consent**: Systems must obtain explicit consent from users before storing their information in long-term memory.
- **Right to Be Forgotten**: Users should have the ability to request deletion of specific memories or all their personal data.
- **Memory Boundaries**: Clear policies must define what information is appropriate to remember and what should be ephemeral.
- **Private vs. Shared Memories**: Systems should distinguish between user-specific memories and knowledge that can be generalized across users.

### Memory Accuracy and Bias

- **False Memories**: Systems might encode incorrect information, requiring mechanisms to verify and correct memories.
- **Selective Memory**: The choice of what to remember can introduce or amplify biases in the system's knowledge.
- **Memory Attribution**: Systems should maintain provenance information for memories to evaluate their reliability.
- **Memory Auditing**: Regular audits should be conducted to identify and address biased or harmful patterns in stored memories.

### Security Considerations

- **Memory Access Control**: Different types of memories may require different levels of access control.
- **Memory Poisoning**: Adversaries might attempt to intentionally introduce false or harmful information into the memory system.
- **Inference Attacks**: Analyzing memory retrieval patterns could potentially reveal sensitive information about users.
- **Memory Encryption**: Sensitive personal memories should be encrypted to prevent unauthorized access.

### Social and Psychological Impact

- **Dependency Risks**: Users might become dependent on AI memory systems, potentially affecting their own memory skills.
- **Relationship Boundaries**: Memory-enabled AI systems may create illusions of genuine relationships that could affect vulnerable users.
- **Trust Calibration**: Users must understand the limitations of AI memory systems to maintain appropriate trust levels.
- **Identity Implications**: Systems that remember user preferences and history might influence users' sense of identity over time.

### Implementation Guidelines

To address these concerns, we recommend:

1. **Transparent Memory Policies**: Clearly communicate what information is stored, for how long, and how it will be used.
2. **Granular Control**: Provide users with easy-to-use controls for reviewing, editing, and deleting specific memories.
3. **Memory Tiering**: Implement different retention policies for different types of information (e.g., factual vs. personal).
4. **Regular Auditing**: Conduct systematic reviews of memory contents to identify and address potential issues.
5. **Responsible Deployment**: Initially limit deployment to applications with lower privacy and safety risks while refining safety measures.

By thoughtfully addressing these ethical considerations, we can develop memory-enhanced LLMs that respect user privacy and autonomy while providing the benefits of long-term personalization and consistency.

## Conclusion

We have explored how insights from the neuroscience of memory – particularly the role of hippocampal cognitive maps – can inspire the next generation of memory systems in LLMs. By reviewing current LLM memory architectures, we found that while techniques like RAG and large context windows extend an LLM's recall to an extent, they fall short of the flexibility, efficiency, and longevity of biological memory. The hippocampal-entorhinal system offers a compelling model: it encodes experiences in a structured form that enables fast relational queries and supports planning through associative links between actions and outcomes. Translating these principles, we proposed an LLM architecture with a dedicated memory module that stores information in a learned cognitive map, enabling semantic and context-dependent retrieval, and even prediction of outcomes for decision support. We discussed how such a system can

 ## Conclusion

We have explored how insights from the neuroscience of memory – particularly the role of hippocampal cognitive maps – can inspire the next generation of memory systems in LLMs. By reviewing current LLM memory architectures, we found that while techniques like RAG and large context windows extend an LLM's recall to an extent, they fall short of the flexibility, efficiency, and longevity of biological memory. The hippocampal-entorhinal system offers a compelling model: it encodes experiences in a structured form that enables fast relational queries and supports planning through associative links between actions and outcomes. Translating these principles, we proposed an LLM architecture with a dedicated memory module that stores information in a learned cognitive map, enabling semantic and context-dependent retrieval, and even prediction of outcomes for decision support. We discussed how such a system can be implemented with existing technology (vector databases, transformers, etc.) and how it compares favorably with purely increasing context length as seen in GPT-4.1, Claude 3.7, and Gemini 2.5. We also enriched the discussion with perspectives from RL (for training and structuring the memory), neuro-symbolic AI (for integrating explicit knowledge), and multi-agent systems (for modular design and memory sharing), painting a picture of a comprehensive cognitive architecture for AI.

The implications of endowing LLMs with a human-like memory system are significant. It could enable AI assistants that truly learn from their interactions and retain knowledge over time, yielding more personalized and context-aware support. It could improve consistency in generated narratives or long analyses, as the model can refer back to earlier content reliably. It might also help address factual accuracy and reduce hallucinations – if the model has a habit of checking its memory for a fact, it's less likely to make one up. Moreover, the ability to form cognitive maps of knowledge could allow LLMs to do something akin to analogical reasoning: seeing structural similarities between one problem and another by comparing their "positions" in the memory space.

There are, of course, challenges and open questions. How do we prevent the memory from growing without bound and picking up noise? What is the best way to integrate the retrieved information into the model's inference (prepend text vs. architectural fusion)? How do we secure the memory (since it could contain sensitive data a user revealed)? Can the model generalize the memory querying behavior to novel situations reliably (the way humans spontaneously recall relevant past events)? Some of these questions will require interdisciplinary research – combining machine learning experiments with cognitive science models of memory usage. Encouragingly, we see a convergence of fields: ideas like cognitive maps and episodic memory are now at the forefront of both neuroscience and AI discussions.

Future work should explore training LLMs with intrinsic memory mechanisms end-to-end. For example, one might pre-train a transformer not just to predict the next word, but also to predict a relevant source from a knowledge corpus (teaching it to retrieve). There is early research in this direction (e.g. the RETRO model by DeepMind, 2022, which fused retrieval into training). Another direction is to leverage the replay and sleep phase idea: have an LLM generate summaries of the day's new information and fine-tune itself on that (simulating offline consolidation). In reinforcement learning agents (like game-playing AIs), adding a learned memory module could help them handle partial observability and long-term dependencies by recalling past states.

While companies like OpenAI, Anthropic and Google are beginning to explore persistent memory features in their commercial offerings, these systems still appear to use simple vector retrieval rather than the rich relational memory structures proposed in this paper. As these features mature, we anticipate growing interest in more sophisticated memory architectures that better mirror human cognitive capabilities.

In conclusion, bridging biological memory systems and artificial ones is a promising path to more intelligent and reliable AI. As one paper succinctly put it, the absence of an extendable memory imposes limits on transformers. By breaking that barrier – giving LLMs a memory that grows and adapts – we move closer to AI that can learn and reason continuously, much like humans who accumulate knowledge throughout life. The hippocampus evolved as nature's solution to encode and navigate large memory spaces; by emulating its design, we may unlock new capabilities for LLMs and pave the way for AI with genuine understanding and foresight grounded in rich memories.

## References

1. Barnaveli, I., Viganò, S., Reznik, D., Haggard, P., & Doeller, C.F. (2025). Hippocampal-entorhinal cognitive maps and cortical motor system represent action plans and their outcomes. Nature Communications, 16, 4139.

2. Ahn, K. (2025). HEMA: A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations. arXiv:2504.16754.

3. OpenAI (2025). Introducing GPT-4.1 in the API. OpenAI Blog, April 2025.

4. Anthropic (2024). Claude 3 Family: Introducing Opus, Sonnet, and Haiku. Anthropic Blog, March 2024.

5. Google (2025). Gemini 2.5 Pro: Enhanced reasoning and extended context. Google AI Blog, March 2025.

6. Park, J.S. et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442.

7. Khandelwal, K., et al. (2020). Generalization through Memorization: Nearest Neighbor Language Models. ICLR 2020.

8. Borgeaud, S., et al. (2022). Improving language models by retrieving from trillions of tokens. ICML 2022.

9. Momennejad, I., et al. (2017). The successor representation in human reinforcement learning. Nature Human Behaviour, 1(9), 680-692.

10. Dayan, P. (1993). Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4), 613-624.

11. Graves, A., et al. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626), 471-476.

12. O'Keefe, J., & Nadel, L. (1978). The hippocampus as a cognitive map. Oxford: Clarendon Press.

13. Tolman, E. C. (1948). Cognitive maps in rats and men. Psychological Review, 55(4), 189–208.

14. Meng, K., et al. (2023). MEMGPT: Towards LLMs as Operating Systems. arXiv:2310.08560.

15. Liu, J., et al. (2023). Learning to Forget: Continual Learning of Large Language Models with Dynamic Memory. arXiv:2310.08231.

16. Gershman, S. J., & Daw, N. D. (2017). Reinforcement learning and episodic memory in humans and animals: An integrative framework. Annual Review of Psychology, 68, 101-128.




